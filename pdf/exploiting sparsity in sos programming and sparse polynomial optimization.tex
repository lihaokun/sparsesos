\documentclass{amsart}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{enumerate}
\usepackage{mathrsfs}
\usepackage{color}
\usepackage{tikz}
\usepackage{appendix}
\usepackage{makecell}

\allowdisplaybreaks[4]

\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
%\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{xca}[theorem]{Exercise}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
%\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\numberwithin{equation}{section}

\def\Z{{\mathbb{Z}}}
\def\Q{{\mathbb{Q}}}
\def\R{{\mathbb{R}}}
\def\C{{\mathbb{C}}}
\def\N{{\mathbb{N}}}
\def\T{{\mathbb{T}}}
\def\Y{{\mathbb{Y}}}
\def\x{{\mathbf{x}}}
\def\z{{\mathbf{z}}}
\def\e{{\mathbf{e}}}
\def\y{{\mathbf{y}}}
\def\bu{{\mathbf{u}}}
\def\bb{{\mathbf{b}}}
\def\bo{{\boldsymbol{\omega}}}
\def\bn{{\boldsymbol{\nu}}}
\def\a{{\boldsymbol{\alpha}}}
\def\dd{{\boldsymbol{\delta}}}
\def\b{{\boldsymbol{\beta}}}
\def\bv{{\boldsymbol{v}}}
\def\A{{\mathscr{A}}}
\def\B{{\mathscr{B}}}

\def\supp{\hbox{\rm{supp}}}
\def\PSD{\hbox{\rm{PSD}}}
\def\SONC{\hbox{\rm{SONC}}}
\def\maximal{\hbox{\rm{max}}}
\def\int{\hbox{\rm{int}}}
\def\New{\hbox{\rm{New}}}
\def\Conv{\hbox{\rm{conv}}}
\def\rank{\hbox{\rm{rank}}}
\def\Cone{\hbox{\rm{cone}}}
\def\modd{\hbox{\rm{mod}}}

\begin{document}

\title{Exploiting Sparsity in SOS Programming and Sparse Polynomial Optimizations}
\thanks{This work was supported partly by NSFC under
grants 61732001 and 61532019.}
\author{Jie Wang}
\address{Jie Wang\\ School of Mathematical Sciences, Peking University}
\email{wangjie212@pku.edu.cn}
\subjclass[2010]{Primary, 14P10,90C25; Secondary, 52B20,12D15}
\keywords{nonnegative polynomial, sparse polynomial, polynomial optimization, sum of squares, chordal graph}
\date{\today}

\begin{abstract}
In this paper, we consider a new pattern of sparsity for SOS Programming named by cross sparsity patterns. We use matrix decompositions for a class of PSD matrices with chordal sparsity patterns to construct sets of supports for a sparse SOS decomposition. The method is applied to the certificate of the nonnegativity of sparse polynomials and unconstrained sparse polynomial optimization problems. Many numerical experiments are given. It turns out that our method can dramatically reduce the computational cost and can handle really huge polynomials, for example, polynomials with $10$ variables, of degree $40$ and more than $5000$ terms.
\end{abstract}

\maketitle
\bibliographystyle{amsplain}

\section{Introduction}
Certificates of nonnegative polynomials and polynomial optimization problems (POPs) arise from many fields such as mathematics, control, engineering, statistics and physics. A classical method for these problems is using sums of squares (SOS) programming which can be effectively solved by semidefinite program (SDP) (\cite{pa,pa1}). However, when the given polynomial has many variables and a high degree, the size of corresponding matrices for SDP is very large and the existing SDP solvers are hard to deal with. On the other hand, most polynomials coming from practice have certain structures including symmetry and sparsity. So it is very important to take full advantage of structures of polynomials to reduce the size of corresponding SDP problems. A lot of work has been done on this subject (\cite{be1,ca,dai,ga,he1,iw,ko,lo1,ma,pe,pe1,re,waki,waki1,waki2,wang,we,yang1}).

For a polynomial $f\in\R[\x]=\R[x_1,\ldots,x_n]$, if we choose a monomial basis $M=\{\x^{\bo_1},\ldots,\x^{\bo_r}\}$, then the SOS condition can be converted to the problem of deciding if there exists a positive semidefinite matrix $Q$ (Gram matrix) such that $f(x)=M^TQM$. There are two approaches to reduce computations. One approach is reducing the size of the monomial basis $M$; such techniques include using Newton polytopes (\cite{re}), the diagonal inconsistency (\cite{lo1}), the iterative elimination method (\cite{ko}), and the facial reduction (\cite{pe,waki1,waki2}). The second approach is exploiting the sparsity of the Gram matrix $Q$; such techniques include the correlative sparsity (\cite{ca,ma,waki,we}), the symmetry property (\cite{ga}), the split property (\cite{dai}), minimal coordinate projections (\cite{pe1}), and the coefficient matching conditions (\cite{be1,he1,yang1}).

In this paper, we consider a new pattern of sparsity for SOS Programming named by {\em cross sparsity patterns}. Given a polynomial $f$ with the support set $\A\subseteq\N^n$, the cross sparsity pattern associated with $\A$ is described in terms of a $r\times r$ symmetric $(0,1)$-matrix $R_{\A}$ whose elements are given by
\begin{equation}
R_{ij}=\begin{cases}
1, &\bo_i+\bo_j\in(2\N)^n\cup\mathscr{A},\\
0, &\textrm{otherwise}.
\end{cases}
\end{equation}

From the cross sparsity pattern matrix $R_{\A}$, we associate it with a undirected graph $G(V_\mathscr{A},E_\mathscr{A})$ with $V_\mathscr{A}=\{1,2,\ldots,r\}$ and $E_\mathscr{A}=\{\{i,j\}\mid i,j\in V_\mathscr{A}, i<j, R_{ij}=1\}$. The key idea in this paper is to use matrix decompositions for a class of positive semidefinite matrices with chordal sparsity patterns to construct sets of supports for a sparse SOS decomposition. Concretely, suppose that $\tilde{G}(V_\mathscr{A},\tilde{E}_\mathscr{A})$ is a chordal extension of $G(V_\mathscr{A},E_\mathscr{A})$, and $C_1, C_2, \ldots, C_t\subseteq V_\mathscr{A}$ denote the maximal cliques of $\tilde{G}(V_\mathscr{A},\tilde{E}_\mathscr{A})$. Let $\mathscr{B}_k=\{\bo_i\mid i\in C_k\}$ for $i=1,2,\ldots,t$. Then we take $f=\sum_{k=1}^tf_k^2$ as a sparse SOS relaxation for the nonnegativity of the sparse polynomial $f$, where $f_k$ has the support set $\B_k$ for $k=1,\ldots,t$. If the size of the cliques $C_k, k=1,\ldots,t$ is small, this can reduce the computational cost. This is somewhat similar to the use of correlative sparsity patterns in \cite{waki}. However, correlative sparsity patterns focus on the sparsity of variables and our cross sparsity patterns are more general.

We test our method on many examples. It turns out that our method can dramatically reduce the computational cost and can handle really huge polynomials, for example, polynomials with $10$ variables, of degree $40$ and more than $5000$ terms.

The rest of the paper organized as follows. In section 2, we introduce some basic notions from nonnegative polynomials and graph theory. In section 3, we define a cross sparsity pattern associated with a sparse polynomial. We show that how we can exploit this sparsity pattern to obtain a sparse SOS relaxation for the nonnegativity of the sparse polynomial. In section 4, we apply this sparse SOS relaxation to unconstrained sparse POPs. We discuss in section 5 when the sparse SOS relaxation obtains the same optimal values as the dense SOS relaxation. Section 6 includes numerical results on various examples. We show that the proposed sparse SOS relaxation exhibits significantly better performance in practice. Finally, we give conclusions in section 7.

\section{Preliminaries}
\subsection{Nonnegative Polynomials}
Let $\R[\x]=\R[x_1,\ldots,x_n]$ be the ring of real $n$-variate polynomial. For a finite set $\A\subset\N^n$, we denote by $\Conv(\A)$ the convex hull of $\A$, and by $V(\A)$ the vertices of the convex hull of $\A$. Also we denote by $V(P)$ the vertex set of a polytope $P$. A polynomial $f\in\R[\x]$ can be written as $f(\x)=\sum_{\a\in\A}c_{\a}\x^{\a}$ with $c_{\a}\in\R, \x^{\a}=x_1^{\alpha_1}\cdots x_n^{\alpha_n}$. The support of $f$ is defined by $\supp(f)=\{\a\in\A\mid c_{\a}\ne0\}$, the degree of $f$ is defined
by $\deg(f)=\max\{\sum_{i=1}^n\alpha_i:\a\in\supp(f)\}$, and the Newton polytope of $f$ is defined as $\New(f)=\Conv(\{\a:\a\in\supp(f)\})$.

A polynomial $f\in\R[\x]$ which is nonnegative over $\R^n$ is called a {\em nonnegative polynomial}. The class of nonnegative polynomials is denoted by PSD, which is a convex cone.

A vector $\a\in\N^n$ is {\em even} if $\alpha_i$ is an even number for $i=1,\ldots,n$. A necessary condition for a polynomial $f(\x)$ to be nonnegative is that every vertex of its Newton polytope is an even vector, i.e. $V(\New(f))=V(\supp(f))\subseteq(2\N)^n$ (\cite{re}).

For a nonempty finite set $\B\subseteq\N^n$, $\R[\B]$ denotes the set of polynomials in $\R[\x]$ whose supports are contained in $\B$, i.e., $\R[\B]=\{f\in\R[\x]\mid\supp(f)\subseteq\B\}$ and we use $\R[\mathscr{B}]^2$ to denote the set of polynomials which are sums of polynomials in $\R[\B]$. Let $\x^{\mathscr{B}}$ be the $|\mathscr{B}|$-dimensional column vector consisting of elements $\x^{\b},\b\in\mathscr{B}$, then
\begin{equation*}
\R[\mathscr{B}]^2=\{(\x^{\mathscr{B}})^TQ\x^{\mathscr{B}}\mid Q\in S_+^{|\B|}\},
\end{equation*}
where the matrix $Q$ is called the Gram matrix.

\subsection{Chordal Graphs}
A {\em graph} $G(V,E)$ consists of a set of nodes $V=\{1,2,\ldots,r\}$ and a set of edges $E\subseteq V\times V$. A graph $G(V,E)$ is said to be {\em undirected} if and only if $(i,j)\in E\Leftrightarrow (j,i)\in E$. A {\em cycle} of length $k$ is a sequence of nodes $\{v_1,v_2,\ldots,v_k\}\subseteq V$ with $(v_k,v_1)\in E$ and $(v_i, v_{i+1})\in E$, for $i=1,\ldots,k-1$. A {\em chord} in a cycle $\{v_1,v_2,\ldots,v_k\}$ is an edge $(v_i v_j)$ that joins two nonconsecutive nodes in the cycle.
\begin{definition}
An undirected graph is {\em chordal} if all its cycles of length at least four have a chord.
\end{definition}
Chordal graphs include some common classes of graphs, such as complete graphs, line graphs and trees. Note that any non-chordal graph $G(V,E)$ can always be extended to a chordal graph
$\tilde{G}(V,\tilde{E})$ by adding appropriate edges to $E$. Finally, we introduce the concept of cliques: a {\em clique} $C\subseteq V$ is a subset of nodes where $(i,j)\in E,\forall i,j\in C,i\ne j$. If a clique $C$ is not a subset of any other clique, then it is called a {\em maximal clique}. It is known that maximal cliques of a chordal graph can be enumerated
efficiently in linear time in the number of vertices and edges of the graph. See \cite{fg,go} for chordal graphs and finding all maximal cliques.

Given an undirected graph $G(V,E)$, we define an extended set of edges $E^{\star}:=E\cup\{(i,i)\mid i\in V\}$ that includes all selfloops. Then, we define the space of symmetric sparse matrices as
\begin{equation}\label{sec2-eq1}
S^r(E,0):=\{X\in S^r\mid X_{ij}=X_{ji}=0\textrm{ if }(i,j)\in E^{\star}\}
\end{equation}
and the cone of sparse PSD matrices as
\begin{equation}\label{sec2-eq2}
S^r_+(E,0):=\{X\in S^r(E,0)\mid X\succeq0\}.
\end{equation}

Given a maximal clique $C_k$, we define a matrix $P_{C_k}\in \R^{|C_k|\times r}$ as
\begin{equation}\label{sec2-eq3}
(P_{C_k})_{ij}=\begin{cases}
1, &C_k(i)=j,\\
0, &\textrm{otherwise}.
\end{cases}
\end{equation}
where $C_k(i)$ denotes the i-th node in $C_k$, sorted in the natural ordering. Note that $X_k=P_{C_k}X_kP_{C_k}^T\in S^{|C_k|}$ extracts a principal submatrix defined by the indices in clique $C_k$, and the operation $E_{C_k}^TX_kE_{C_k}$ inflates a $|C_k|\times|C_k|$ matrix into a sparse $r\times r$ matrix. Then, the following results characterize, respectively, the membership to the set $S^r_+(E,0)$ when the underlying graph $G(V,E)$ is chordal.
\begin{theorem}[\cite{ag}]\label{sec2-thm}
Let $G(V,E)$ be a chordal graph with maximal cliques $\{C_1,\ldots,C_t\}$. Then $X\in S_+^r(E,0)$ if and only if there exist $X_k\in S_+^{|C_k|},k=1,\ldots,t$ such that $X=\sum_{k=1}^tP_{C_k}^TX_kP_{C_k}$.
\end{theorem}

\section{Exploiting sparsity in SOS programming}
A basic problem that appears in many fields is checking global nonnegativity of multivariate polynomials. This is difficult in general. A convenient approach for this, originally introduced by Parrilo in \cite{pa}, is the use of sums of squares as a suitable replacement for nonnegativity. Given a polynomial $f(\x)\in\R[\x]$, if there exist polynomials $f_1(\x),\ldots,f_m(\x)$ such that
\begin{equation}\label{sec3-eq3}
f(\x)=\sum_{i=1}^mf_i(\x)^2,
\end{equation}
then we say $f(\x)$ is a {\em sum of squares} (SOS). The existence of an SOS decomposition of a given polynomial gives a certificate for its global nonnegativity. For $d\in\N$, let $\N^n_d:=\{\a\in\N^n\mid\sum_{i=1}^n\alpha_i\le d\}$ and assume $f\in\R[\N^n_{2d}]$. The SOS condition (\ref{sec3-eq3}) can be converted to the problem of deciding if there exists a positive semidefinite matrix $Q$ such that
\begin{equation}\label{sec3-eq7}
f(\x)=(\x^{\N^n_{d}})^TQ\x^{\N^n_{d}},
\end{equation}
which can be efficiently solved by a semidefinite programming problem (SDP).

We say that a polynomial $f\in\R[\N^n_{2d}]$ is {\em sparse} if the number of elements in its support $\A=\supp(f)$ is much smaller than the number of elements in $\N^n_{2d}$ that forms a support of fully dense polynomials in $\R[\N^n_{2d}]$. When $f(\x)$ is a sparse polynomial in $\R[\N^n_{2d}]$, the size of the SDP problem (\ref{sec3-eq7}) can be reduced by eliminating redundant elements from $\N^n_{d}$. In fact, $\N^n_{d}$ in problem (\ref{sec3-eq7}) can be replaced by (\cite{re})
\begin{equation}\label{sec3-eq8}
\B=\Conv(\{\frac{\a}{2}\mid\a\in V(\A)\})\cap\N^n\subseteq\N^n_{d}.
\end{equation}
There are also other methods to reduce the size of $\B$ further (\cite{ko,pe,waki1}). However, we assume in this paper that $\B$ is as (\ref{sec3-eq8}).

\subsection{Cross Sparsity Pattern}
Let $f(\x)\in\R[\x]$ with $\supp(f)=\mathscr{A}$. Assume that $\B$ is as (\ref{sec3-eq0}) and $\B=\{\bo_1,\ldots,\bo_r\}$. The sparsity considered in this paper is measured by the different kinds of cross product of monomials arising in the objective polynomial $f(\x)$. It is represented by a $r\times r$ {\em cross sparsity pattern matrix} $\mathbf{R}_{\mathscr{A}}=(R_{ij})$ whose elements are given by
\begin{equation}\label{sec3-eq1}
R_{ij}=\begin{cases}
1, &\bo_i+\bo_j\in(2\N)^n\cup\mathscr{A},\\
0, &\textrm{otherwise}.
\end{cases}
\end{equation}

Given a cross sparsity pattern matrix $\mathbf{R}_\mathscr{A}=(R_{ij})$, the graph $G(V_\mathscr{A},E_\mathscr{A})$ with $V_\mathscr{A}=\{1,2,\ldots,r\}$ and $E_\mathscr{A}=\{\{i,j\}\mid i,j\in V_\mathscr{A}, i<j, R_{ij}=1\}$ is called the {\em cross sparsity pattern graph}. To apply Theorem \ref{sec2-thm}, we generate a chordal extension $\tilde{G}(V_\mathscr{A},\tilde{E}_\mathscr{A})$ of the cross sparsity pattern graph $G(V_\mathscr{A},E_\mathscr{A})$ and use the extended
cross sparsity pattern graph $\tilde{G}(V_\mathscr{A},\tilde{E}_\mathscr{A})$ instead of $G(V_\mathscr{A},E_\mathscr{A})$.

Given a graph $G(V_\mathscr{A},E_\mathscr{A})$, there may be many different chordal extensions and choosing anyone of them is valid for deriving the sparse relaxation presented in this paper. For example, we can take all of the connected components of $G(V_\mathscr{A},E_\mathscr{A})$ and add edges such that every connected component becomes a complete subgraph. The obtained graph will be a simple choice for chordal extensions. The chordal extension with the least number of edges is called the {\em minimum chordal extension}. The minimum chordal extension serves as the best choice for the resulting sparse relaxation. However, finding the minimum chordal extension of a graph is an NP-hard problem in general. Finding a chordal extension of a graph is equivalent to calculating the symbolic sparse Cholesky factorization of its adjacency matrix. The resulted sparse matrix represents a chordal extension. The minimum chordal extension corresponds to the sparse Cholesky factorization with the minimum fill-ins. Fortunately, several heuristic algorithms, such as the minimum degree ordering, are known to efficiently produce a good approximation. For more information on symbolic Cholesky factorizations with the minimum degree ordering and minimum chordal extensions, please refer to \cite{am,be,he}.

\subsection{Sparse SOS relaxations}\label{sec-ssos}
Given $\A\subseteq\N^n$ with $V(\A)\subseteq(2\N)^n$, $\B$ is as (\ref{sec3-eq8}). Let the set of SOS polynomials supported on $\A$ be
\begin{equation*}
\Sigma(\mathscr{A}):=\{f\in\R[\A]\mid\exists Q\in S_+^r\textrm{ s.t. }f=(\x^{\mathscr{B}})^TQ\x^{\mathscr{B}}\}.
\end{equation*}
Generally the Gram matrix $Q$ for a sparse SOS polynomial $f(\x)$ can be dense. To maintain the sparsity of $f(\x)$ in the Gram matrix $Q$, we consider a subset of SOS polynomials
\begin{equation*}
\tilde{\Sigma}(\mathscr{A}):=\{f\in\R[\A]\mid\exists Q\in S_+^r(\tilde{E}_{\mathscr{A}},0)\textrm{ s.t. }f=(\x^{\mathscr{B}})^TQ\x^{\mathscr{B}}\}.
\end{equation*}

With this restriction, we have the following result.
\begin{theorem}\label{sec3-thm1}
Given $\A\subseteq\N^n$ with $V(\A)\subseteq(2\N)^n$, assume that $\B=\{\bo_1,\ldots,\bo_r\}$ is as (\ref{sec3-eq8}) and the cross sparsity pattern graph is $\tilde{G}(V_\mathscr{A},\tilde{E}_\mathscr{A})$. Let $C_1, C_2, \ldots, C_t\subseteq V_\mathscr{A}$ denote the maximal cliques of $\tilde{G}(V_\mathscr{A},\tilde{E}_\mathscr{A})$ and $\mathscr{B}_k=\{\bo_i\in\B\mid i\in C_k\}, i=1,2,\ldots,t$. Then, $f(\x)\in\tilde{\Sigma}(\mathscr{A})$ if and only if there exist $f_k(\x)\in\R[\mathscr{B}_k], k=1,\ldots,t$ such that
\begin{equation}\label{sec3-eq9}
f(\x)=\sum_{k=1}^tf_k(\x)^2.
\end{equation}
\end{theorem}
\begin{proof}
By Theorem \ref{sec2-thm}, $Q\in S_+^r(\tilde{E}_{\mathscr{A}},0)$ if and only if there exist $Q_k\in S_+^{|C_k|},k=1,\ldots,t$ such that $Q=\sum_{k=1}^tP_{C_k}^TQ_kP_{C_k}$. So $f(\x)\in\tilde{\Sigma}(\mathscr{A})$ if and only if there exist $Q_k\in S_+^{|C_k|},k=1,\ldots,t$ such that
\begin{align*}
f(\x)&=(\x^{\mathscr{B}})^T(\sum_{k=1}^tP_{C_k}^TQ_kP_{C_k})\x^{\mathscr{B}}\\
&=\sum_{k=1}^t(P_{C_k}\x^{\mathscr{B}})^TQ_k(P_{C_k}\x^{\mathscr{B}})\\
&=\sum_{k=1}^t(\x^{\mathscr{B}_k})^TQ_k\x^{\mathscr{B}_k},
\end{align*}
which is equivalent to that there exist $f_k(\x)\in\R[\mathscr{B}_k], k=1,\ldots,t$ such that $f(\x)=\sum_{k=1}^tf_k(\x)^2$.
\end{proof}

\section{Sparse polynomial optimization}
We consider the unconstrained polynomial optimization problem:
\begin{equation}\label{sec4-eq1}
\textrm{minimize}\quad f(\x).
\end{equation}

We first convert the POP (\ref{sec4-eq1}) into an equivalent problem,
\begin{equation}\label{sec3-eq00}
\begin{cases}
\textrm{maximize} &\xi\\
\textrm{subject to} &f(\x)-\xi\ge0.
\end{cases}
\end{equation}
Let $\xi^*$ denote the optimal value of (\ref{sec3-eq00}). Assume $f\in\R[\N^n_{2d}]$. Then we can replace the constraint of the problem (\ref{sec3-eq00}) by an SOS constraint to obtain
\begin{equation}\label{sec3-eq01}
\begin{cases}
\textrm{maximize} &\xi\\
\textrm{subject to} &f(\x)-\xi\in\R[\N^n_{d}]^2.
\end{cases}
\end{equation}
Let $\xi_{sos}^*$ denote the optimal value of (\ref{sec3-eq01}). The SOS optimization problem (\ref{sec3-eq01}) serves as a relaxation of the POP (\ref{sec3-eq00}). Note that we can rewrite the SOS constraint of (\ref{sec3-eq01}) as $f(\x)-\xi=(\x^{\N^n_{d}})^TQ\x^{\N^n_{d}}$ and $Q\in S_+^{|\N^n_{d}|}$.

When the objective function $f(\x)$ is a sparse polynomial in $\R[\N^n_{2d}]$, the SOS constraint of (\ref{sec3-eq01}) can be replaced by $f(\x)-\xi\in\R[\B]^2$ with
\begin{equation}\label{sec3-eq0}
\B=\Conv(\{\frac{\a}{2}\mid\a\in\supp(f)\}\cup\{\mathbf{0}\})\cap\N^n\subseteq\N^n_{d}.
\end{equation}
Note that $\mathbf{0}$ is added as the support for the real number variable $\xi$. Therefore, we obtain
\begin{equation}\label{sec3-eq02}
\begin{cases}
\textrm{maximize} &\xi\\
\textrm{subject to} &f(\x)-\xi\in\R[\B]^2.
\end{cases}
\end{equation}

Let $\A=\supp(f)\cup\{\mathbf{0}\}$. We rewrite the SOS optimization problem (\ref{sec3-eq01}) as
\begin{equation}\label{sec3-eq4}
\begin{cases}
\textrm{maximize} &\xi\\
\textrm{subject to} &f(\x)-\xi\in\Sigma(\mathscr{A}).
\end{cases}
\end{equation}
To exploit the sparsity, we replace the constraint $f(\x)-\xi\in\Sigma(\mathscr{A})$ by the stronger constraint $f(\x)-\xi\in\tilde{\Sigma}(\mathscr{A})$ to obtain
\begin{equation}\label{sec3-eq5}
\begin{cases}
\textrm{maximize} &\xi\\
\textrm{subject to} &f(\x)-\xi\in\tilde{\Sigma}(\mathscr{A}).
\end{cases}
\end{equation}
Let $\xi_{ssos}^*$ denote the optimal value of (\ref{sec3-eq5}). Assume that $\B=\{\bo_1,\ldots,\bo_r\}$ and the cross sparsity pattern graph is $\tilde{G}(V_\mathscr{A},\tilde{E}_\mathscr{A})$. Let $C_1, C_2, \ldots, C_t\subseteq V_\mathscr{A}$ denote the maximal cliques of $\tilde{G}(V_\mathscr{A},\tilde{E}_\mathscr{A})$ and $\mathscr{B}_k=\{\bo_i\in\B\mid i\in C_k\}, i=1,2,\ldots,t$. Then Theorem \ref{sec3-thm1} allows us to decompose the single large SOS constraint $f(\x)-\xi\in\tilde{\Sigma}(\mathscr{A})$ into a set of SOS constraints with smaller dimensions,
\begin{equation}\label{sec3-eq6}
\begin{cases}
\textrm{maximize} &\xi\\
\textrm{subject to} &f(\x)-\xi\in\sum_{k=1}^t\R[\mathscr{B}_k]^2.
\end{cases}
\end{equation}
This can reduce the computational cost significantly if the size of the cliques $C_k, k=1,\ldots,t$ is small.

The relation between the optimums of the polynomial optimization problem (\ref{sec3-eq00}), the SOS optimization problem (\ref{sec3-eq01}) and the sparse SOS optimization problem (\ref{sec3-eq5}) are
$$\xi^*\ge\xi_{sos}^*\ge\xi_{ssos}^*.$$

\section{When do $\Sigma(\mathscr{A})$ and $\tilde{\Sigma}(\mathscr{A})$ coincide}
Given $\A\subseteq\N^n$ with $V(\A)\subseteq(2\N)^n$, we define in Section \ref{sec-ssos} two sets of nonnegative polynomials: $\Sigma(\mathscr{A})$ and $\tilde{\Sigma}(\mathscr{A})$. Generally we have $\Sigma(\mathscr{A})\supseteq\tilde{\Sigma}(\mathscr{A})$. There are certain cases in which $\Sigma(\mathscr{A})=\tilde{\Sigma}(\mathscr{A})$ holds.\smallskip

1.The quadratic case\smallskip

Suppose $f\in\R[\x]$ is a quadratic polynomial. Let $M=[1,x_1,\ldots,x_n]$ be a monomial basis and assume $f=M^TQM$ for a positive semidefinite matrix $Q=(q_{ij})$. Let $\mathbf{R}=(R_{ij})$ be the corresponding cross sparsity pattern matrix for $f$. If $x_i$ not appears in $f$, then we must have $q_{i0}=q_{0i}=0$ and $R_{i0}=R_{0i}=0$. If $x_ix_j$ not appears in $f$, then we must have $q_{ij}=q_{ji}=0$ and $R_{ij}=R_{ji}=0$. Therefore, $Q\in S_+^{n+1}(\tilde{E}_{\mathscr{A}},0)$, where $\mathscr{A}=\supp(f)$. So in this case we have $\Sigma(\mathscr{A})=\tilde{\Sigma}(\mathscr{A})$.\smallskip

2.$\A\subseteq(2\N)^n$\smallskip

Assume $\B=\{\bo_1,\ldots,\bo_r\}$ is as (\ref{sec3-eq8}). In this case, the elements of the cross sparsity pattern matrix $\mathbf{R}_{\mathscr{A}}$ satisfy
\begin{equation}\label{sec5-eq1}
R_{ij}=\begin{cases}
1, &\bo_i+\bo_j\in(2\N)^n,\\
0, &\textrm{otherwise}.
\end{cases}
\end{equation}
The corresponding cross sparsity pattern graph $G(V_\mathscr{A},E_\mathscr{A})$ has $t$ connected components $C_1, C_2, \ldots, C_t$, everyone of which is a complete subgraph. Moreover, $i,j$ belong to the same connected component if and only if $\bo_i+\bo_j\in(2\N)^n$.

Suppose $f(\x)\in\Sigma(\mathscr{A})$. We have $f(x_1,\ldots,x_i,\ldots,x_n)=f(x_1,\ldots,-x_i,\ldots,x_n)$ for $i=1,\ldots,n$. It follows that the polynomial $f(\x)$ has $n$ sign-symmetries defined by the $n$ standard basis vectors $\e_1,\ldots,\e_n$. Therefore, by Theorem 3 of \cite{lo1}, the monomials $\B$ can be block partitioned into $t$ blocks $\B_1,\ldots,\B_t$ where $\bo_i,\bo_j$ belong to the same block if and only if $\bo_i+\bo_j\in(2\N)^n$, such that $f(\x)\in\sum_{k=1}^t\R[\mathscr{B}_k]^2$. Then $\mathscr{B}_k=\{\bo_i\in\B\mid i\in C_k\}, i=1,2,\ldots,t$. So $f(\x)\in\tilde{\Sigma}(\mathscr{A})$ and hence $\Sigma(\mathscr{A})=\tilde{\Sigma}(\mathscr{A})$.

\begin{remark}
In \cite{lo1}, sign-symmetries is exploited to block diagonalize sums of squares programming (Theorem 3). It is easy to show that our blocking decompositions are always a refinement of the block-diagonalizations obtained by sign-symmetries.
\end{remark}

%\section{Comparison with minimal coordinate projections}
%Permenter and Parrilo found block-diagonalizations for SOS programming using minimal coordinate projections in \cite{pe}. We compare the block-diagonalizations by minimal coordinate projections and the blocking decompositions by our method, and point out that our blocking decompositions are always a refinement of the block-diagonalizations by minimal coordinate projections.
%\begin{lemma}\label{sec6-lm1}
%Given $f(\x)\in\R[\N^n_{2d}]$, let $I:=\{i\in[n]\mid \alpha_i\textrm{ is even }, \forall\a\in\supp(f)\}$. Then using minimal coordinate projections, the monomials $\x^{\bo}$ and $\x^{\bn}$ belong to the same block if and only if $\omega_i\equiv\nu_i(\modd~2)$ for all $i\in I$.
%\end{lemma}
%\begin{proof}
%Suppose $P_M$ is the minimal coordinate projection of the SDP formulated from the SOS condition of $f(\x)$ and $R$ is the relation corresponding $M$. The rows and columns of $M$ are labeled by the monomials $\{\x^{\bo}\mid\bo\in\N^n_{d}\}$. For $i\in I$, if the only monomial involved $x_i$ in $f$ is $c_ix_i^{2d}$, then $\{x_i^d\}$ forms a block and we omit this case. For simplicity, we assume $i=n$. Choose a monomial $c_{\a}\x^{\a}$ involved $x_n$ in $f$ and assume $\a'=(\alpha_1,\ldots,\alpha_{n-1})$, $\alpha_n=2d_n$, $0<d_n<d$. For any $\bu_1,\bu_2\in\N^{n-1}_{d}$ such that $\bu_1+\bu_2=\a'$, we have $((\bu_1,d_n-1),(\bu_2,d_n+1)),((\bu_1,d_n+1),(\bu_2,d_n-1))\in R$. By the transitivity of $R$, we have $((\bu_1,d_n-1),(\bu_2,d_n-1)),((\bu_1,d_n+1),(\bu_2,d_n+1))\in R$. Thus for any $\bo,\bn\in\N^{n}_{d}$ such that $\bo+\bn=(\a',2d_n-2)$ or $\bo+\bn=(\a',2d_n+2)$, we have $(\bo,\bn)\in R$. Continue this process and eventually we obtain that for any $\bo,\bn\in\N^{n}_{d}$ such that $\bo+\bn=(\a',2d')$, $0\le d'\le d$, $(\bo,\bn)\in R$.
%
%For $i\notin I$, we assume $i=1$. There must exist a monomial $c_{\a}\x^{\a}$ involved $x_1$ in $f$ with $\alpha_1=2d_1-1$, $1\le d_1\le d$. Let $\a'=(\alpha_2,\ldots,\alpha_{n})$. For any $\bu_1,\bu_2\in\N^{n-1}_{d}$ such that $\bu_1+\bu_2=\a'$, we have $((d_1-1,\bu_1),(d_1,\bu_2)),((d_1,\bu_1),(d_1-1,\bu_2))\in R$. By the transitivity of $R$, we have $((d_1-1,\bu_1),(d_1-1,\bu_2)),((d_1,\bu_1),(d_1,\bu_2))\in R$. Thus for any $\bo,\bn\in\N^{n}_{d}$ such that $\bo+\bn=(2d_1-2,\a')$ or $\bo+\bn=(2d_1,\a')$, we have $(\bo,\bn)\in R$. It follows that for any $\bu_1,\bu_2\in\N^{n-1}_{d}$ such that $\bu_1+\bu_2=\a'$, we have $((d_1,\bu_1),(d_1-1,\bu_2)),((d_1,\bu_1),(d_1-2,\bu_2))\in R$ and $((d_1-1,\bu_1),(d_ 1,\bu_2)),((d_1-1,\bu_1),(d_1+1,\bu_2))\in R$. By the transitivity of $R$, we have
%\end{proof}

\section{Algorithms and Numerical results}
In this section, we give examples and numerical results to illustrate the effectiveness of our method. It turns out that our method is extremely powerful and can deal with really huge polynomials that cannot be handled by other methods.

\subsection{Algorithms}
Determining cross sparse SOS polynomial can be easily divided into 4 steps:
\begin{enumerate}
  \item Computing support of SOS;
  \item Generate cross sparsity pattern graph;
  \item Computing the triangulation of graphs and get Blocked;
  \item Using SDP solver.
\end{enumerate}

We simply use the computing connectivity branch to computing the triangulation. Here are some of the results of the polynomial. Note that because there are several ways to computing support of SOS, we do not calculate the time to computing support of SOS.

(CPU:Intel i7-4760HQ@2.10GHz(core 4,thread 8), memory:16G, SYSTEM:ARCH LINUX, SDP Solved:CSDP 6.2.0)

\subsection{SOS programming}
\subsubsection{}
Let $B_m=(\sum_{i=1}^{3m+2}x_i^2)((\sum_{i=1}^{3m+2}x_i^2)^2-2\sum_{i=1}^{3m+2}x_i^2\sum_{j=1}^mx_{i+3j+1}^2)$, where $x_{3m+2+r}=x_r$. Note that $B_m$ is modified from \cite{pa}. For any $m\in\N^*$, $B_m$ is homogeneous and is a SOS polynomial. For these $B_m$'s, our algorithm SparseSOS dramatically reduces the problem sizes and the solving times (Table 1).

\begin{table}[htbp]
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
      \hline
       m & \#Basis & \#Block & SOS& SparseSOS  \\
      \hline
      1 & 35 & $5\times 5,10\times 1$ & 0.03s & 0.01s\\
      \hline
      2 & 120 & $8\times 8,56\times 1$ & 0.88s & 0.04s\\
      \hline
      3 & 286 & $11\times 11,165\times 1$ & 38.90s & 0.08s\\
      \hline
      4 & 560 & $14\times 14,364\times 1$ & 1001.40s & 0.24s\\
      \hline
      5 & 969 & $17\times 17,680\times 1$ & OOM & 0.65s\\
      \hline
      6 & 1540 & $20\times 20,1140\times 1$ & OOM & 1.63s\\
      \hline
    \end{tabular}
  \end{center}
  \caption{Sizes of PSD constraints and solving times before and after using SparseSOS for $B_m$'s. The notion $i\times j$ represents $i$ blocks of size $j$. The notion OOM indicates an out-of-memory error.}
\end{table}

\subsubsection{}
Monotone Column Permanent (MCP) Conjecture was given in \cite{ha}. In the dimension $4$, this conjecture is equivalent to decide whether particular polynomials named by $p_{1,2}, p_{1,3}, p_{2,2}, p_{2,3}$ are nonnegative (the definitions of $p_{i,j}$ can be found in \cite{ka}). Actually, it was proved that every $p_{i,j}$ multiplied by a small particular polynomial is a SOS polynomial (\cite{ka}). Let
\begin{align*}
&P_{1,2}=(a^2 + 2 b^2 + c^2)\cdot p_{1,2},\\
&P_{1,3}=p_{1,3},\\
&P_{2,2}=(a^2 + 2 b^2 + c^2)\cdot p_{2,2},\\
&P_{2,3}=(a^2 + 2 b^2 + c^2)\cdot p_{2,3}.
\end{align*}
We use our algorithm SparseSOS to certify the nonnegativity of $P_{1,2}, P_{1,3}, P_{2,2}, P_{2,3}$. The result is listed in Table 2.

\begin{table}[htbp]
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
      \hline
       &\#Support& \#Basis & \#Block &SOS&SparseSOS\\
      \hline
      $P_{1,2}$ & 159 &77 & $15,2\times 12,7\times 4,3,2\times 2,3\times 1$ & 0.29s & 0.05s\\
      \hline
      $P_{1,3}$ & 53 &29 & $8,4\times 3,2\times 2,5\times 1$ & 0.29s & 0.02s\\
      \hline
      $P_{2,2}$ & 144 &62 & $3\times 12,2\times 4,8\times 2,2\times 1$ & 0.24s & 0.07s\\
      \hline
      $P_{2,3}$ & 107 &53 & $2\times 10,8,4,3,8\times 2,2\times 1$ & 0.12s & 0.05s\\
      \hline
    \end{tabular}
  \end{center}
  \caption{Sizes of PSD constraints and solving times before and after using SparseSOS for $P_{1,2}, P_{1,3}, P_{2,2}, P_{2,3}$.}  
\end{table}

\subsubsection{}
The following polynomial Vor1 appears in \cite{ev}. It was proved that Vor1 is nonnegative and its discriminant (denoted by Vor2) is also nonnegative. The polynomial Vor2 has $1571$ monomials and is of degree $30$.
\begin{align*}
\textrm{Vor1}=&16 a^2(\alpha^2 + 1 + \beta^2)u^4 + 16 a(-\alpha \beta a^2 + ax\alpha + 2 a\alpha^2 + 2 a + 2 a\beta^2 + ay\beta-\\
&\alpha \beta)u^3 + ((24 a^2 + 4 a^4)\alpha^2 + (-24 \beta a^3 - 24 a\beta - 8 ya^3 + 24 xa^2 - 8 ay)\\
&\alpha + 24 a^2\beta^2 + 4 \beta^2 - 8 \beta xa^3 + 4 y^2a^2 + 24 y\beta a^2 - 8 ax\beta + 16 a^2 + 4 x^2a^2)\\
&u^2 + (-4 \alpha a^3 + 4 ya^2 - 4 ax - 8 a\alpha + 8 \beta a^2 + 4 \beta)(\beta - a\alpha + y - ax)u+\\
&(a^2 + 1)(\beta - a\alpha + y - ax)^2
\end{align*}
\begin{table}[htbp]
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|c|}
      \hline
       &\#Support& \#Basis & \#Block &SOS&SparseSOS\\
      \hline
      $\textrm{Vor1}$ &63&18 & $2\times 8,1\times 2$ & 0.01s & 0.01s\\
      \hline
      $\textrm{Vor2}$ &1571&597 & $121,88,2\times 70,2\times 64,2\times 60$ & 390.13s & 2.18s\\
      \hline
    \end{tabular}
  \end{center}
  \caption{Sizes of PSD constraints and solving times before and after using SparseSOS for $\textrm{Vor1},\textrm{Vor2}$.}  
\end{table}

\subsubsection{}
The polynomials $J_{40},J_{421},J_{50},J_{521}$ (see the Appendix) are given by Jeffrey Uhlmann. The sizes of $J_{40},J_{421},J_{50},J_{521}$ are listed in Table 3. One can see that $J_{40},J_{50}$ are really huge and the corresponding SDPs are unsolvable. However, our algorithm SparseSOS can handle with them in less than one minute (Table 4).
\begin{table}[htbp]
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
      \hline
       &\#Support&dim& deg    \\
      \hline
      $J_{40}$ & 138 & 6 & 12 \\
      \hline
      $J_{421}$ & 116 & 6 & 12\\
      \hline
      $J_{50}$ & 5687 & 10 & 20\\
      \hline
      $J_{521}$ & 5157 & 10 & 20\\
      \hline
    \end{tabular}
  \end{center}
  \caption{Scales of $J_{40}$,$J_{421}$,$J_{50}$,$J_{521}$. The first column is the number of supports; the second column is the number of variables; the third column is the degrees. }

\end{table}
\begin{table}[htbp]
  \begin{center}
    \begin{tabular}{|c|c|c|c|c|}
      \hline
       &\#Basis &\#Block &SOS& SparseSOS  \\
      \hline
      $J_{40}$ &64&$14,3\times 10,2\times 6,5,3 $&0.24s&0.04s\\
      \hline
      $J_{421}$ &48&$12,10,7,4,3,2,3\times 1$&0.13s&0.04s\\
      \hline
      $J_{50}$ &1014&\makecell{$121 ,94 ,92 ,86 ,84 ,64 ,62,55,$\\
      $2\times 52 ,51 ,49 ,45 ,40 ,39 ,28$}
      &OOM&44.65s\\
      \hline
      $J_{521}$ &864&\makecell{$111 ,80 ,77 ,76 ,75 ,55 ,54,52,$\\
      $43, 2\times 42 ,39,2\times 34 ,30 ,20$}
      &OOM&35.23s\\
      \hline
    \end{tabular}
  \end{center}
  \caption{Sizes of PSD constraints and solving times before and after using SparseSOS for $J_{40}$,$J_{421}$,$J_{50}$,$J_{521}$.}

\end{table}

\subsection{Unconstrained polynomial optimization}
%\subsubsection{}
%The following polynomial appears in \cite{em}. One has to decide if this polynomial is always positive. We compute its infimum to answer this question.
%\begin{align*}
%\textrm{IT}=&1 - 8 c^2d^2 - 196608 e^3a^2d^2c + 1536 ead^4c^2 + 21504 e^2ad^2c - 4096 e^2ac^3d^2-\\
%&384 ead^2+ 1024 e^2ac + 16 c^4d^4 - 72 c^2d^4 + 1024 c^2e^2 + 36864 e^2a^2d^4-3456 ead^4\\
%&+ 262144 e^4a^2c^2 - 32768 e^3ac^2 + 256 c^3d^2e-576 ced^2 + 81 d^4 + 64 ce - 18 d^2.
%\end{align*}

%\subsubsection{}
%The following polynomial appears in \cite{ka1}, which is positive semidefinite but cannot be written as a polynomial SOS.
%\begin{align*}
%\textrm{LL5}=&a^4 + 3 a^3 b + 3 a^2 b^2 + 2 a^3 c + 4 a^2 b c + a^2 c^2 + a b c^2 +b^2 c^2 + a^3 d + 2 a^2 b d + a^2 c d \\
% &+ a b c d + b^2 c d + a b d^2 +b^2 d^2 + 2 a c d^2 + 4 b c d^2 + 3 c^2 d^2 + a d^3 + 2 b d^3 +3 c d^3 \\
% &+ d^4.
%\end{align*}
%$(a^2+b^2+c^2+d^2)\cdot\textrm{LL5}$ is a SOS.
%
%
%Compare $\xi_{sos}^*$ and $\xi_{ssos}^*$.

\subsection{Comparison with minimal coordinate projections}
Permenter and Parrilo found block-diagonalizations for SOS programming using minimal coordinate projections in \cite{pe}. We compare the block-diagonalizations by minimal coordinate projections and the blocking decompositions by our method. In most cases, our blocking decompositions are a refinement of the block-diagonalizations by minimal coordinate projections.

Compare the size of blocks through the above examples.

\section{Conclusions}

\begin{thebibliography}{99}
\bibliographystyle{amsplain}
\bibitem{ag}
J. Agler, W. Helton, S. McCullough, L. Rodman, {\em Positive semidefinite matrices with a given sparsity pattern}, Linear algebra and its applications, 107(1988):101-149.

\bibitem{am}
P. R. Amestoy, T. A. Davis, I. S. Duff, {\em Algorithm 837: AMD, an approximate minimum degree ordering algorithm}, ACM Transactions on Mathematical Software, 30(3)(2004):381-388.

\bibitem{be}
A. Berry, J. R. S. Blair, P. Heggernes, B. W. Peyton, {\em  Maximum cardinality search for computing minimal triangulations of graphs}, Algorithmica, 39(4)(2004):287-298.

\bibitem{be1}
D. Bertsimas, R. M. Freund, X. A. Sun, {\em An accelerated first-order method for solving SOS relaxations of unconstrained polynomial optimization problems}, Optim. Methods Softw., 28(3)(2013):424-441.

\bibitem{bp}
J. R. S. Blair, B. Peyton, {\em An introduction to chordal graphs and clique trees}, in Graph Theory and Sparse Matrix Computation, A. George, J. R. Gilbert, and J. W. H. Liu, eds.,
Springer-Verlag, New York, 1993:1每29.

\bibitem{ca}
J. S. Campos, P. Parpas, {\em A Multigrid Approach to SDP Relaxations of Sparse Polynomial Optimization Problems}, Siam Journal on Optimization, 28(1)2016:1-29.

\bibitem{dai}
L. Dai, B. Xia, {\em Smaller SDP for SOS decomposition}, Journal of Global Optimization, 63(2)(2015):343-361.

\bibitem{em}
I. Z. Emiris, E. P. Tsigaridas, {\em Real algebraic numbers and polynomial systems of small degree}, Theoretical Computer Science, 409(2)(2008):186-199.

\bibitem{ev}
H. Everett, D. Lazard, S. Lazard, M. Safey El Din, {\em The voronoi diagram of three lines}, Discrete and Computational Geometry, 42(1)(2009):94-130.

\bibitem{fg}
D. R. Fulkerson, O. A. Gross, {\em Incidence matrices and interval graphs}, Pacific J. Math., 15(1965):835-855.

\bibitem{ga}
K. Gatermann, P. A. Parrilo, {\em Symmetry groups, semidefinite programs, and sums of squares}, Journal of Pure and Applied Algebra, 192(1)(2002):95-128.

\bibitem{go}
M. C. Golumbic, {\em Algorithmic Graph Theory and Perfect Graphs}, Academic Press, New York, 1980.

\bibitem{he}
P. Heggernes, {\em Minimal triangulations of graphs: a survey}, Discrete Mathematics, 306(3)(2006):297-317.

\bibitem{he1}
D. Henrion, J. Malick, {\em Projection methods in conic optimization}, in Handbook on Semidefinite, Conic and Polynomial Optimization, New York, NY, USA: Springer, 2012:565-600.

\bibitem{iw}
S. Iliman, T. de Wolff, {\em Amoebas, nonnegative poltnomials and sums of squares supported on circuits}, Res. Math. Sci., 3(2016), 3:9.

\bibitem{ko}
M. Kojima, S. Kim, H. Waki, {\em Sparsity in sums of squares of polynomials}, Math. Program., 103(2005):45-62.

\bibitem{lo}
J. L\"ofberg, YALMIP: a toolbox for modeling and optimization in MATLAB, In 2004 IEEE International Conference on Robotics and Automation (IEEE Cat. No.04CH37508), 284-289.

\bibitem{lo1}
J. L\"ofberg, {\em Pre- and Post-Processing Sum-of-Squares Programs in Practice}, IEEE Transactions on Automatic Control, 54(5)(2009):1007-1011.

\bibitem{ha}
J. Haglund, K. Ono, D. G. Wagner, {\em Theorems and conjectures involving rook polynomials with real roots}, In: Proceedings of Topics in Number Theory and Combinatorics, 1997:207每221.

\bibitem{ka1}
E. Kaltofen, B. Li, Z. Yang, L. Zhi, {\em Exact certification in global polynomial optimization via sums-of-squares of rational functions with rational coefficients}, Journal of Symbolic Computation, 47(1)(2012):1-15.

\bibitem{ka}
E. Kaltofen, Z. Yang, L. Zhi, {\em A proof of themonotone column permanent (mcp) conjecture for dimension 4 via sums-of-squares of rational functions}, In: Proceedings of the 2009 Conference on Symbolic Numeric Computation, SNC ＊09, 2009:65每70, ACM, New York.

\bibitem{li}
B. Li, J. Nie, L. Zhi, {\em Approximate GCDs of polynomials and sparse SOS relaxations}, Theoretical Computer Science, 409(2)(2008):200-210.

\bibitem{ma}
A. Marandi, E. D. Klerk, J. Dahl, {\em Solving sparse polynomial optimization problems with chordal structure using the sparse bounded-degree sum-of-squares hierarchy}, Discrete Applied Mathematics, 2017.

\bibitem{pa}
P. A. Parrilo, {\em Structured semidefinite programs and semialgebraic geometry methods in robustness and optimization}, Ph.D. Thesis, California Institute of Technology, 2000.

\bibitem{pa1}
P. A. Parrilo, B. Sturmfels, {\em Minimizing Polynomial Functions}, Proceedings of the Dimacs Workshop on Algorithmic and Quantitative Aspects of Real Algebraic Geometry in Mathematics and Computer Science, 32(1)(2001):83-100.

\bibitem{pe}
F. Permenter, P. A. Parrilo, {\em Basis selection for SOS programs via facial reduction and polyhedral approximations}, Decision and Control. IEEE, 2014:6615-6620.

\bibitem{pe1}
F. Permenter, P. A. Parrilo, {\em Finding sparse, equivalent SDPs using minimal coordinate projections}, In 54th IEEE Conference on Decision and Control, CDC 2015, Osaka, Japan,
December 15-18, 2015:7274每7279.

\bibitem{re}
B. Reznick, {\em Extremal PSD forms with few terms}, Duke Math. J., 45(1978):363-374.

\bibitem{va}
L. Vandenberghe, M. S. Andersen, {\em Chordal Graphs and Semidefinite Optimization}, Now Publisher, 1900.

\bibitem{waki}
H. Waki, S. Kim, M. Kojima, M. Muramatsu, {\em Sums of squares and semidefinite program relaxations for polynomial optimization problems with structured sparsity}, SIAM Journal on Optimization, 17(1)(2016):218-242.

\bibitem{waki1}
H. Waki, M. Muramatsu, {\em A facial reduction algorithm for finding sparse SOS representations}, Operations Research Letters, 38(5)(2009):361-365.

\bibitem{waki2}
H. Waki, M. Muramatsu, {\em An extension of the elimination method for a sparse SOS polynomial}, Journal of the Operations Research Society of Japan, 4(4)(2017):161-190.

\bibitem{wang}
J. Wang, {\em Nonnegative Polynomials and Circuit Polynomials}, 2018, arXiv:1804.09455.

\bibitem{we}
T. Weisser, J. B. Lasserre, K. C. Toh, {\em Sparse-BSOS: a bounded degree SOS hierarchy for large scale polynomial optimization with sparsity}, Mathematical Programming Computation, 10(1)(2018):1-32.

\bibitem{yang}
Z. Yang, G. Fantuzzi, A. Papachristodoulou, {\em Decomposition and completion of sum-of-squares matrices}, 2018, arXiv:1804.02711.

\bibitem{yang1}
Z. Yang, G. Fantuzzi, A. Papachristodoulou, {\em Exploiting Sparsity in the Coefficient Matching Conditions in Sum-of-Squares Programming Using ADMM}, IEEE Control Systems Letters, 1(1)(2017):80-85.

\end{thebibliography}
\appendix
\appendixpage
\begin{appendix}
\begin{align*}
J_{521}=&(a g + h) (-h + c i + e j -c f j) ((b d + c e + f) (-f + i j -f j^2)(-(b d + c e + f)g\\
        &(-g + a h + b i - a c i + d j -a e j - b f j + a c f j)(-f + i j - f j^2) +d (b g + c h +\\
        &i)(-i + f j) (-d + a e + b f - a c f + g j -a h j - b i j + a c i j - d j^2 + a e j^2 + b f j^2 \\
        &-a c f j^2)) - (1 + d^2 + e^2 + f^2)(1 +j^2) (-(1 + b^2 + c^2) g (-g + a h + b i - a c i \\
        &+ d j - a e j - b f j + a c f j) (1 + f^2 + i^2 - 2 f i j +f^2 j^2) +b (b g + c h + i) (-i + \\
        &f j) (-b + a c + d f - a e f - b f^2 +a c f^2 + g i - a h i - b i^2 + a c i^2 - f g j + a f h j \\
        &-d i j + a e i j + 2 b f i j - 2 a c f i j + d f j^2 -a e f j^2 - b f^2 j^2 + a c f^2 j^2)) -j (d g + \\
        &e h + f i + j) (-(1 + b^2 + c^2) d (-d + a e + b f - a c f + g j - a h j -b i j + a c i j - \\
        &d j^2 + a e j^2 + b f j^2 -a c f j^2) (1 + f^2 + i^2 - 2 f i j + f^2 j^2) + b (b d + c e + f) (-f \\
        &+ i j - f j^2) (-b + a c + d f - a e f -b f^2 + a c f^2 + g i - a h i - b i^2 + a c i^2 - f g j \\
        &+a f h j - d i j + a e i j + 2 b f i j - 2 a c f i j +d f j^2 - a e f j^2 - b f^2 j^2 + a c f^2 j^2))) - \\
        &(b g +c h + i) (-i + f j) ((a d + e) (-e + c f + h j - c i j - e j^2 +c f j^2) (-(b d + c e \\
        &+ f) g (-g + a h + b i - a c i + d j -a e j - b f j + a c f j) (-f + i j - f j^2) +d (b g + \\
        &c h + i) (-i + f j) (-d + a e + b f - a c f + g j -a h j - b i j + a c i j - d j^2 + a e j^2 + \\
        &b f j^2 -a c f j^2)) - (1 + d^2 + e^2 + f^2) (1 +j^2) (-(a b + c) g (-g + a h + b i - a c i + \\
        &d j - a e j -b f j + a c f j) (-c + e f - c f^2 + h i - c i^2 - f h j -e i j + 2 c f i j + e f j^2\\
        & - c f^2 j^2) + a (b g + c h + i) (-i + f j) (-a + b c - a c^2 + d e - a e^2 -c d f - b e f + \\
        &2 a c e f + b c f^2 - a c^2 f^2 + g h -a h^2 - c g i - b h i + 2 a c h i + b c i^2 - a c^2 i^2 -e g j + \\
        &c f g j - d h j + 2 a e h j + b f h j -2 a c f h j + c d i j + b e i j - 2 a c e i j -2 b c f i j + 2 a c^2 \\
        &f i j + d e j^2 - a e^2 j^2 - c d f j^2 - b e f j^2 + 2 a c e f j^2 + b c f^2 j^2 -a c^2 f^2 j^2)) -j (d g +\\
        & e h + f i +j) (-(a b + c) d (-d + a e + b f - a c f + g j - a h j -b i j + a c i j - d j^2 +\\
        & a e j^2 + b f j^2 -a c f j^2) (-c + e f - c f^2 + h i - c i^2 - f h j -e i j + 2 c f i j + e f j^2 - \\
        &c f^2 j^2) +a (b d + c e + f) (-f + i j - f j^2) (-a + b c - a c^2 + d e -a e^2 - c d f - b e \\
        &f + 2 a c e f + b c f^2 - a c^2 f^2 + g h - a h^2 - c g i - b h i + 2 a c h i + b c i^2 -a c^2 i^2 - e g \\
        &j + c f g j - d h j + 2 a e h j + b f h j -2 a c f h j + c d i j + b e i j - 2 a c e i j -2 b c f i j + \\
        &2 a c^2 f i j + d e j^2 - a e^2 j^2 -c d f j^2 - b e f j^2 + 2 a c e f j^2 + b c f^2 j^2 -a c^2 f^2 j^2))) -j \\
        &(d g + e h + f i +j) ((a d + e) (-e + c f + h j - c i j - e j^2 +c f j^2) (-(1 + b^2 + c^2) \\
        &g (-g + a h + b i - a c i + d j -a e j - b f j + a c f j) (1 + f^2 + i^2 - 2 f i j +f^2 j^2) +\\
        &b (b g + c h + i) (-i + f j) (-b + a c + d f - a e f - b f^2 +a c f^2 + g i - a h i - b i^2 + \\
        &a c i^2 - f g j + a f h j -d i j + a e i j + 2 b f i j - 2 a c f i j + d f j^2 - a e f j^2 - b f^2 j^2 +\\
        &a c f^2 j^2)) - (b d + c e + f) (-f + i j -f j^2) (-(a b + c) g (-g + a h + b i - a c i + d j\\
        & - a e j - b f j + a c f j) (-c + e f - c f^2 + h i - c i^2 - f h j -e i j + 2 c f i j + e f j^2 - \\
        &c f^2 j^2) +a (b g + c h + i) (-i + f j) (-a + b c - a c^2 + d e - a e^2 -c d f - b e f + 2 a c \\
        &e f + b c f^2 - a c^2 f^2 + g h -a h^2 - c g i - b h i + 2 a c h i + b c i^2 - a c^2 i^2 - e g j + c f g j \\
        &- d h j + 2 a e h j + b f h j -2 a c f h j + c d i j + b e i j - 2 a c e i j - 2 b c f i j + 2 a c^2 f i j +\\
        & d e j^2 - a e^2 j^2 -c d f j^2 - b e f j^2 + 2 a c e f j^2 + b c f^2 j^2 -a c^2 f^2 j^2)) -j (d g + e h + f i\\
        & + j) (-b (a b + c) (-c + e f - c f^2 + h i - c i^2 - f h j -e i j + 2 c f i j + e f j^2 - c f^2 j^2) \\
        &(-b + a c + d f -a e f - b f^2 + a c f^2 + g i - a h i - b i^2 + a c i^2 -f g j + a f h j - d i j\\
        & + a e i j + 2 b f i j -2 a c f i j + d f j^2 - a e f j^2 - b f^2 j^2 + a c f^2 j^2) +  a (1 + b^2 + c^2) (1 \\
        &+ f^2 + i^2 - 2 f i j + f^2 j^2) (-a + b c - a c^2 + d e - a e^2 - c d f - b e f + 2 a c e f + b c f^2 \\
        &- a c^2 f^2 + g h - a h^2 - c g i - b h i +2 a c h i + b c i^2 - a c^2 i^2 - e g j + c f g j - d h j +\\
        &2 a e h j + b f h j - 2 a c f h j + c d i j + b e i j -2 a c e i j - 2 b c f i j + 2 a c^2 f i j + d e j^2 -a e^2\\
        &j^2 - c d f j^2 - b e f j^2 + 2 a c e f j^2 +b c f^2 j^2 - a c^2 f^2 j^2))) - (1 + g^2 + h^2 + i^2 +j^2)\\
        & ((a d + e) (-e + c f + h j - c i j - e j^2 + c f j^2) (-(1 + b^2 + c^2) d (-d + a e + b f - \\
        &a c f + g j - a h j - b i j + a c i j - d j^2 + a e j^2 + b f j^2 -a c f j^2) (1 + f^2 + i^2 - 2 f i j \\
        &+ f^2 j^2) +b (b d + c e + f) (-f + i j - f j^2) (-b + a c + d f - a e f -b f^2 + a c f^2 + \\
        &g i - a h i - b i^2 + a c i^2 - f g j +a f h j - d i j + a e i j + 2 b f i j - 2 a c f i j +d f j^2 - a \\
        &e f j^2 - b f^2 j^2 + a c f^2 j^2)) - (b d +c e + f) (-f + i j -f j^2)(-a + b c - a c^2 + d e \\
        &- a e^2 - c d f - b e f + 2 a c e f +b c f^2 - a c^2 f^2 + g h - a h^2 - c g i - b h i + 2 a c h i +\\
        &b c i^2 - a c^2 i^2 - e g j + c f g j - d h j + 2 a e h j +b f h j - 2 a c f h j + c d i j + b e i j - 2 a \\
        &c e i j -2 b c f i j + 2 a c^2 f i j + d e j^2 - a e^2 j^2 - c d f j^2 -b e f j^2 + 2 a c e f j^2 + b c f^2 j^2 \\
        &- a c^2 f^2 j^2)) + (1 + d^2 + e^2 + f^2) (1 +j^2) (-b (a b + c) (-c + e f - c f^2 + h i - \\
        &c i^2 - f h j -e i j + 2 c f i j + e f j^2 - c f^2 j^2) (-b + a c + d f -a e f - b f^2 + a c f^2 +\\
        & g i - a h i - b i^2 + a c i^2 -f g j + a f h j - d i j + a e i j + 2 b f i j - 2 a c f i j + d f j^2 -\\
        & a e f j^2 - b f^2 j^2 + a c f^2 j^2) +a (1 + b^2 + c^2) (1 + f^2 + i^2 - 2 f i j + f^2 j^2) (-a + \\
        &b c -a c^2 + d e - a e^2 - c d f - b e f + 2 a c e f + b c f^2 -a c^2 f^2 + g h - a h^2 - c g i - \\
        &b h i + 2 a c h i +b c i^2 - a c^2 i^2 - e g j + c f g j - d h j + 2 a e h j +b f h j - 2 a c f h j + c d\\
        & i j + b e i j - 2 a c e i j -2 b c f i j + 2 a c^2 f i j + d e j^2 - a e^2 j^2 -c d f j^2 - b e f j^2 + 2 a c e \\
        &f j^2 + b c f^2 j^2 -a c^2 f^2 j^2)))
\end{align*}

\end{appendix}
\end{document}
